{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Test of Facial Keypoint Detector\n",
    "## Without PCA and an SVM for each class\n",
    "\n",
    "First we need to identify the keypoint detector model that we will be using. \n",
    "\n",
    "MTCNN - Tensorflow\n",
    "https://github.com/ipazc/mtcnn\n",
    "(Appears to be pretrained)\n",
    "It appears to provide a few facial keypoints. May be useful for patch based approach probably not for direct-point position approach.\n",
    "\n",
    "Pytorch-Facial-Keypoints - Pytorch\n",
    "https://github.com/fiyero/pytorch_facial_keypoints\n",
    "(Not pretrained)\n",
    "Generates keypoints around face but excludes ears. Can be used for direct-point position approach.\n",
    "\n",
    "Openface - in Lua?\n",
    "https://github.com/cmusatyalab/openface\n",
    "(Might be pretrained)\n",
    "Not sure if it generates keypoints\n",
    "\n",
    "Openface - \n",
    "https://github.com/TadasBaltrusaitis/OpenFace\n",
    "(Pretrained models)\n",
    "High number of facial points. Can be used for direct-point position approach.\n",
    "\n",
    "Facial-Landmark\n",
    "https://github.com/huohuotm/Facial-Landmark\n",
    "(pretrained models)\n",
    "High number of facial points. Can be used for direct-point position approach.\n",
    "\n",
    "Face-Recognition\n",
    "https://github.com/ageitgey/face_recognition\n",
    "(Is pretrained and comes as a library)\n",
    "Detects facial points.\n",
    "\n",
    "## Preprocessing images\n",
    "TODO ?\n",
    "\n",
    "## Processing on keypoints\n",
    "TODO ?\n",
    "\n",
    "## For the classifier\n",
    "https://scikit-learn.org/stable/modules/svm.html\n",
    "Recommend Linear SVC first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Plan\n",
    "I am planning on using Face-Recognition which provides a wrapper around dlib face-keypoint-detector. (If we wanted to we could write our own).\n",
    "\n",
    "Program will read in image, run face-recog for key-points, and store key-points in a CSV file.\n",
    "\n",
    "Then using the key-points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second approach is an end-to-end solution\n",
    "\n",
    "Something akin to https://github.com/ZER-0-NE/ML_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 800, 3)\n",
      "[{'chin': [(339, 265), (322, 302), (309, 343), (299, 386), (297, 428), (303, 470), (316, 507), (332, 541), (360, 563), (395, 573), (434, 566), (470, 552), (501, 532), (529, 506), (551, 477), (575, 445), (596, 411)], 'left_eyebrow': [(372, 251), (395, 244), (419, 255), (440, 272), (455, 294)], 'right_eyebrow': [(496, 323), (525, 329), (551, 340), (571, 360), (578, 385)], 'nose_bridge': [(459, 338), (445, 364), (432, 390), (418, 417)], 'nose_tip': [(385, 417), (394, 429), (407, 441), (424, 444), (442, 446)], 'left_eye': [(384, 287), (401, 285), (419, 295), (424, 320), (408, 315), (390, 304)], 'right_eye': [(491, 364), (511, 358), (530, 370), (538, 391), (522, 390), (503, 380)], 'top_lip': [(350, 454), (370, 456), (390, 464), (400, 472), (415, 476), (434, 488), (451, 500), (442, 498), (410, 485), (395, 481), (384, 474), (357, 459)], 'bottom_lip': [(451, 500), (425, 509), (400, 507), (385, 501), (372, 493), (358, 477), (350, 454), (357, 459), (382, 480), (393, 487), (408, 492), (442, 498)]}]\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "image = face_recognition.load_image_file(\"./data/raw/Test/22q11/image_10008_jpg.jpg\")\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "print(image.shape)\n",
    "print(face_landmarks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(\"./data/raw/Test/Angelman/image_161_jpg.jpg\")\n",
    "\n",
    "# Find all facial features in all the faces in the image\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "pil_image = Image.fromarray(image)\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    d = ImageDraw.Draw(pil_image, 'RGBA')\n",
    "\n",
    "    # Make the eyebrows into a nightmare\n",
    "    d.polygon(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 128))\n",
    "    d.polygon(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 128))\n",
    "    d.line(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 150), width=5)\n",
    "    d.line(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 150), width=5)\n",
    "\n",
    "    # Gloss the lips\n",
    "    d.polygon(face_landmarks['top_lip'], fill=(150, 0, 0, 128))\n",
    "    d.polygon(face_landmarks['bottom_lip'], fill=(150, 0, 0, 128))\n",
    "    d.line(face_landmarks['top_lip'], fill=(150, 0, 0, 64), width=8)\n",
    "    d.line(face_landmarks['bottom_lip'], fill=(150, 0, 0, 64), width=8)\n",
    "\n",
    "    # Sparkle the eyes\n",
    "    d.polygon(face_landmarks['left_eye'], fill=(255, 255, 255, 30))\n",
    "    d.polygon(face_landmarks['right_eye'], fill=(255, 255, 255, 30))\n",
    "\n",
    "    # Apply some eyeliner\n",
    "    d.line(face_landmarks['left_eye'] + [face_landmarks['left_eye'][0]], fill=(0, 0, 0, 110), width=6)\n",
    "    d.line(face_landmarks['right_eye'] + [face_landmarks['right_eye'][0]], fill=(0, 0, 0, 110), width=6)\n",
    "\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08551853 0.39414771 0.49848086 0.37514369]]\n",
      "[0.28417571]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_features=4, random_state=0)\n",
    "clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf.fit(X, y)\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image and generate landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "\n",
    "def load_dataset(setPath):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label in ['22q11','Angelman','Apert','CDL','Down','FragileX','Marfan','Progeria','Sotos','TreacherCollins','Turner','Williams']:\n",
    "        directory = os.path.join(setPath,label)\n",
    "        for f in os.listdir(directory):\n",
    "            path = os.path.join(directory,f)\n",
    "            \n",
    "            if os.path.isfile(path) and f != 'desktop.ini':\n",
    "                \n",
    "                # Extract landmarks\n",
    "                image = face_recognition.load_image_file(path)\n",
    "                face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "                # Iterate through detected faces\n",
    "                keypoints = []\n",
    "                for face in face_landmarks_list:\n",
    "\n",
    "                    # Get face center\n",
    "                    # and convert points into list form\n",
    "                    cX = 0\n",
    "                    cY = 0\n",
    "                    count = 0\n",
    "                    points = []\n",
    "\n",
    "                    for featureType in face.keys():\n",
    "                        for point in face[featureType]:\n",
    "                            points = points + list(point)\n",
    "\n",
    "                            cY = cY + point[0]\n",
    "                            cX = cX + point[1]\n",
    "                            count = count + 1\n",
    "                    cX = cX / count\n",
    "                    cY = cY / count\n",
    "\n",
    "                    # Center all points\n",
    "                    for i in range(0,len(points)):\n",
    "                        if i % 2 == 0:\n",
    "                            points[i] = points[i] - cY\n",
    "                        else:\n",
    "                            points[i] = points[i] - cX\n",
    "\n",
    "                    # Store result\n",
    "                    keypoints.append(points)\n",
    "\n",
    "                # Append to dataset\n",
    "                for facePoints in keypoints:\n",
    "                    X.append(facePoints)\n",
    "                    y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 144)\n",
      "(312,)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_dataset('./data/raw/Test')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1022, 144)\n",
      "(1022,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_dataset('./data/raw/Train')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
